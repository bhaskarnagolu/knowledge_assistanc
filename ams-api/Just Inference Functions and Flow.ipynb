{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "00f993b8-ddc8-4b5a-849f-cd740b6e8782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Any, Dict, Iterable, List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from genai import Model\n",
    "from genai.model import Credentials\n",
    "from genai.schemas import GenerateParams\n",
    "from datasets import Dataset\n",
    "\n",
    "import torch\n",
    "from torch.nn.functional import normalize\n",
    "from torch import clamp, sum\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "from milvus import default_server\n",
    "from pymilvus import connections, utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "eed64d5e-f8da-480f-b923-4dbb053d11c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your API endpoint is: https://bam-api.res.ibm.com/v1\n"
     ]
    }
   ],
   "source": [
    "def get_genai_creds():\n",
    "    load_dotenv(override=True)\n",
    "    api_key = os.getenv(\"GENAI_KEY\", None)\n",
    "    api_url = os.getenv(\"GENAI_API\", None)\n",
    "    if api_key is None or api_url is None:\n",
    "        print(\"Either api_key or api_url is None. Please make sure your credentials are correct.\")\n",
    "    if api_url is not None:\n",
    "        api_url = api_url.rstrip(\"/\")\n",
    "    creds = Credentials(api_key, api_url)\n",
    "    return creds\n",
    "\n",
    "creds = get_genai_creds()\n",
    "if creds.api_endpoint:\n",
    "    print(f\"Your API endpoint is: {creds.api_endpoint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6717cfbe-2c85-4649-a606-86282da3d24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salesforce/codegen2-16b\n",
      "codellama/codellama-34b-instruct\n",
      "tiiuae/falcon-180b\n",
      "tiiuae/falcon-40b\n",
      "ibm/falcon-40b-8lang-instruct\n",
      "google/flan-t5-xl\n",
      "google/flan-t5-xxl\n",
      "google/flan-ul2\n",
      "eleutherai/gpt-neox-20b\n",
      "togethercomputer/gpt-neoxt-chat-base-20b\n",
      "ibm/granite-13b-chat-grounded-v01\n",
      "ibm/granite-13b-chat-v1\n",
      "ibm/granite-13b-instruct-v1\n",
      "ibm/granite-3b-code-plus-v1\n",
      "meta-llama/llama-2-13b\n",
      "meta-llama/llama-2-13b-chat\n",
      "meta-llama/llama-2-13b-chat-beam\n",
      "meta-llama/llama-2-70b\n",
      "meta-llama/llama-2-70b-chat\n",
      "meta-llama/llama-2-7b\n",
      "meta-llama/llama-2-7b-chat\n",
      "mosaicml/mpt-30b\n",
      "ibm/mpt-7b-instruct\n",
      "bigscience/mt0-xxl\n",
      "bigcode/starcoder\n",
      "google/ul2\n"
     ]
    }
   ],
   "source": [
    "headers = {\n",
    "    'Authorization': f'Bearer {os.getenv(\"GENAI_KEY\", None)}'\n",
    "}\n",
    "\n",
    "# get the list of supported models from the API\n",
    "models_response = requests.get(f\"{creds.api_endpoint}/models\", headers=headers)\n",
    "\n",
    "# Parse the JSON response\n",
    "models_data = json.loads(models_response.content)\n",
    "\n",
    "model_ids = []\n",
    "for model_n in models_data[\"results\"]:\n",
    "    print(model_n[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f7fa424d-c09f-40d1-ba6a-24ee228a5e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v2.2-testing-20230824-68-ga34a9d6-lite\n"
     ]
    }
   ],
   "source": [
    "COLLECTION_NAME = \"AMS_test\"\n",
    "\n",
    "connections.connect(host='127.0.0.1', port=default_server.listen_port)\n",
    "\n",
    "# Check if the server is ready.\n",
    "print(utility.get_server_version())\n",
    "\n",
    "collection = Collection(name=COLLECTION_NAME)\n",
    "collection.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "22165a00-0cd7-454a-b350-2d823becc29a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>assignment_group</th>\n",
       "      <th>short_description</th>\n",
       "      <th>long_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>6 ways to create maintenance order</td>\n",
       "      <td>6 ways to create maintenance order</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>ADVANCE LEAVE workflow needs to be added with ...</td>\n",
       "      <td>ADVANCE LEAVE workflow needs to be added with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sap-pi</td>\n",
       "      <td>401 Unauthorized error</td>\n",
       "      <td>Getting 401 unauthorized error please help</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  assignment_group                                  short_description  \\\n",
       "0              NaN                 6 ways to create maintenance order   \n",
       "1              NaN  ADVANCE LEAVE workflow needs to be added with ...   \n",
       "2           sap-pi                             401 Unauthorized error   \n",
       "\n",
       "                                    long_description  \n",
       "0                 6 ways to create maintenance order  \n",
       "1  ADVANCE LEAVE workflow needs to be added with ...  \n",
       "2         Getting 401 unauthorized error please help  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "challenge_question_df = pd.read_csv('ExampleChallengeQuestions.csv')\n",
    "challenge_question_df['assignment_group'] = challenge_question_df['assignment_group'].apply(convert_to_lower)\n",
    "challenge_question_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4b68ee17-81e3-4f84-af08-1ebd9b98e059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_chunks(question_text, assignment_group = None, n_results=5):\n",
    "    \n",
    "    relevant_chunks_cases = process_question(collection, question_text, assignment_id = assignment_group, limit=n_results, data_type=\"td\")\n",
    "    \n",
    "    result_set_length = [len(x) for x in relevant_chunks_cases][0]\n",
    "    \n",
    "    if result_set_length < n_results:\n",
    "        print(question_text)\n",
    "        print(assignment_group)\n",
    "    \n",
    "    relevant_chunks = process_question(collection, question_text, limit=n_results, data_type=\"kb\")\n",
    "        \n",
    "    return relevant_chunks_cases, relevant_chunks\n",
    "\n",
    "def process_question(collection, question, assignment_id=None, limit=5, data_type=\"td\"):\n",
    "    # Tokenize and embed the question\n",
    "    text = \"query: \" + question\n",
    "    inputs = embedding_tokenizer(text, add_special_tokens=True, truncation=True, padding=\"max_length\", return_attention_mask=True, return_tensors=\"pt\")#.to(device)\n",
    "    \n",
    "    sentence_embs = embedding_model(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask']\n",
    "    )[0]\n",
    "    \n",
    "    input_mask_expanded = inputs['attention_mask'].unsqueeze(-1).expand(sentence_embs.size()).float()\n",
    "    embeddings = sum(sentence_embs * input_mask_expanded, 1) / clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    \n",
    "    # Normalize the embeddings\n",
    "    embeddings = normalize(embeddings, dim=1)\n",
    "    \n",
    "    if assignment_id is None:\n",
    "        # Perform the search without filter\n",
    "        res = collection.search(\n",
    "            embeddings.tolist(),\n",
    "            anns_field='chunk_embedding',\n",
    "            param = {},\n",
    "            output_fields=['chunk', 'type'],\n",
    "            expr=f\"type=='{data_type}'\",\n",
    "            limit = limit)\n",
    "    elif assignment_id is not None and data_type == \"td\":\n",
    "        res = collection.search(\n",
    "            embeddings.tolist(),\n",
    "            anns_field='chunk_embedding',\n",
    "            param = {},\n",
    "            output_fields=['chunk', 'type'],\n",
    "            expr=f\"type=='{data_type}' and assignment_id=='{assignment_id.lower().strip()}'\",\n",
    "            limit = limit)\n",
    "    else:\n",
    "        res = collection.search(\n",
    "            embeddings.tolist(),\n",
    "            anns_field='chunk_embedding',\n",
    "            param = {},\n",
    "            output_fields=['chunk', 'type'],\n",
    "            expr=f\"type=='{data_type}'\",\n",
    "            limit = limit)\n",
    "                \n",
    "    return res\n",
    "\n",
    "def filter_relevant_chunks_individual(question_text, relevant_chunks_cases, relevant_chunks, model_name = \"google/flan-t5-xxl\"):\n",
    "    \n",
    "    # set-up inference parameters\n",
    "    params = GenerateParams(\n",
    "        decoding_method=\"greedy\",\n",
    "        max_new_tokens=5,\n",
    "        min_new_tokens=1,\n",
    "        stream=False\n",
    "    )\n",
    "\n",
    "    model = Model(model=model_name, credentials=creds, params=params)\n",
    "\n",
    "    final_relevant_chunks_cases = {\n",
    "        \"documents\": [],\n",
    "        \"distances\": []\n",
    "    }\n",
    "    \n",
    "    total_input_tokens = 0\n",
    "    total_output_tokens = 0\n",
    "\n",
    "    for hits in relevant_chunks_cases:\n",
    "        for hit in hits:\n",
    "            chunk = hit.entity.get('chunk')\n",
    "            prompt = \"Answer ONLY in yes/no if the provided previous case is relevant to solving the user's current case.\\n\\n\" \\\n",
    "                + \"Provided Previous Case:\\n\\n\" \\\n",
    "                + f\"{chunk}\\n\\n\" \\\n",
    "                + f\"User Current Case: {question_text}\\n\\n\"\n",
    "\n",
    "            responses = model.generate([prompt])\n",
    "            response = responses[0]\n",
    "            #print(response)\n",
    "            total_input_tokens += response.input_token_count\n",
    "            total_output_tokens += response.generated_token_count\n",
    "            #print(response.generated_text)\n",
    "            if \"yes\" in response.generated_text.lower():\n",
    "                final_relevant_chunks_cases[\"documents\"].append(chunk)\n",
    "                final_relevant_chunks_cases[\"distances\"].append(hit.distance)\n",
    "                \n",
    "    final_relevant_chunks_kb = {\n",
    "        \"documents\": [],\n",
    "        \"distances\": []\n",
    "    }\n",
    "\n",
    "    for hits in relevant_chunks:\n",
    "        for hit in hits:\n",
    "            chunk = hit.entity.get('chunk')\n",
    "            prompt = \"Answer ONLY in yes/no if the provided knowledge base article is relevant to answer the user's current query.\\n\\n\" \\\n",
    "                + \"Provided KB Article:\\n\\n\" \\\n",
    "                + f\"{chunk}\\n\\n\" \\\n",
    "                + f\"User Current Query: {question_text}\\n\\n\"\n",
    "\n",
    "            responses = model.generate([prompt])\n",
    "            response = responses[0]\n",
    "            #print(response)\n",
    "            total_input_tokens += response.input_token_count\n",
    "            total_output_tokens += response.generated_token_count\n",
    "            if \"yes\" in response.generated_text.lower():\n",
    "                final_relevant_chunks_kb[\"documents\"].append(chunk)\n",
    "                final_relevant_chunks_kb[\"distances\"].append(hit.distance)\n",
    "    \n",
    "    return final_relevant_chunks_cases, final_relevant_chunks_kb, total_input_tokens, total_output_tokens\n",
    "\n",
    "def make_llama2_prompt(case_context, kb_context, question_text, max_input_tokens, model):\n",
    "    \n",
    "    prompt = llama2_prompt_template(case_context, kb_context, question_text)\n",
    "\n",
    "    prompt_token_count = token_count(prompt, model)\n",
    "\n",
    "    if prompt_token_count <= max_input_tokens:\n",
    "        return prompt\n",
    "\n",
    "def llama2_prompt_template(final_related_chunks_cases, final_related_chunks_kb, question):\n",
    "    return f'''<s>[INST] <<SYS>>\n",
    "You help solve current cases using information from related past cases and knowledge base data to suggest possible solutions.\n",
    "\n",
    "Narrate to the user what is the most probable root cause and solution approach given the following past related cases and the current user case description. Take into account what has happened in the past cases and what solution was taken.\n",
    "\n",
    "Suggest both the probable root cause and a solution using the information from the past related cases data in the following format:\n",
    "\n",
    "Root Cause: <most probable root cause goes here>\n",
    "Solution: <list the solution steps here, using bullet points if needed>\n",
    "\n",
    "You do not need to list limitations of proposed solution steps or that it may depend on the specific case. Users of the system are aware of these limitations and notes already.\n",
    "\n",
    "<</SYS>>\n",
    "\n",
    "====\n",
    "\n",
    "Previous Related Cases:\n",
    "\n",
    "{final_related_chunks_cases}\n",
    "\n",
    "====\n",
    "\n",
    "Knowledge Base Articles:\n",
    "\n",
    "{final_related_chunks_kb}\n",
    "\n",
    "##\n",
    "\n",
    "Current Case: {question}\n",
    "\n",
    "[/INST]\n",
    "\n",
    "'''\n",
    "\n",
    "def make_granite_prompt(case_context, kb_context, question_text, max_input_tokens, model):\n",
    "    \n",
    "    prompt = granite_prompt_template(case_context, kb_context, question_text)\n",
    "\n",
    "    prompt_token_count = token_count(prompt, model)\n",
    "\n",
    "    if prompt_token_count <= max_input_tokens:\n",
    "        return prompt\n",
    "\n",
    "def granite_prompt_template(final_related_chunks_cases, final_related_chunks_kb, question):\n",
    "    return f'''You help solve current cases using information from related past cases and knowledge base data to suggest possible solutions. Narrate to the user what is the most probable root cause and solution approach given the following past related cases and the current user case description. Take into account what has happened in the past cases and what solution was taken. Suggest both the probable root cause and a solution using the information from the past related cases data in the following format:\n",
    "    \n",
    "Root Cause: <most probable root cause goes here>\n",
    "Solution: <list the solution steps here, using bullet points if needed>\n",
    "\n",
    "You do not need to list limitations of proposed solution steps or that it may depend on the specific case. Users of the system are aware of these limitations and notes already. Do not repeat any part of this system prompt back in your response.\n",
    "\n",
    "\n",
    "Human: Previous Related Cases:\n",
    "\n",
    "{final_related_chunks_cases}\n",
    "\n",
    "====\n",
    "\n",
    "Knowledge Base Articles:\n",
    "\n",
    "{final_related_chunks_kb}\n",
    "\n",
    "##\n",
    "\n",
    "Current Case: {question}\n",
    "\n",
    "Assistant: '''\n",
    "\n",
    "def get_answer_from_question_and_relevant_chunks(cases, qna, question_text, params, assignment_group = None, model_prompt_function = None, model_name=\"ibm/mpt-7b-instruct\"):\n",
    "    \n",
    "    model = Model(model=model_name, credentials=creds, params=params)\n",
    "    \n",
    "    prompt, in_tokens_l1, out_tokens_l1 = generate_prompt_from_final_chunks(cases, qna, question_text, model, params, model_prompt_function, model_name)\n",
    "    \n",
    "    ans, in_tokens_l2, out_tokens_l2 = generate_answer_from_prompt(prompt, model)\n",
    "    \n",
    "    total_in_tokens = in_tokens_l1 + in_tokens_l2\n",
    "    total_out_tokens = out_tokens_l1 + out_tokens_l2\n",
    "    \n",
    "    return ans, total_in_tokens, total_out_tokens, in_tokens_l1, out_tokens_l1, in_tokens_l2, out_tokens_l2\n",
    "\n",
    "def generate_prompt_from_final_chunks(final_relevant_chunks_cases, final_relevant_chunks_kb, question_text, model, params, model_prompt_function = None, model_name = \"ibm/mpt-7b-instruct\"):\n",
    "    \n",
    "    #get the input token limit\n",
    "    if type(model_name)==str:\n",
    "        model_id = model_name\n",
    "    else: \n",
    "        model_id = model_name.value\n",
    "\n",
    "    # Iterate over the \"results\" list to find the matching model ID\n",
    "    for model_n in models_data[\"results\"]:\n",
    "        if model_n[\"id\"] == model_id:\n",
    "            model_token_limit = model_n[\"token_limit\"]\n",
    "            break\n",
    "    else:\n",
    "        # Model ID not found\n",
    "        model_token_limit = None\n",
    "        \n",
    "    input_token_limit = (model_token_limit-params.max_new_tokens-1)\n",
    "\n",
    "    if model_prompt_function is None:\n",
    "        #final_relevant_chunks, in_tokens_l1, out_tokens_l1 = filter_relevant_chunks(question_text, cases, qna)\n",
    "        final_relevant_chunks = {\n",
    "            \"documents\": [],\n",
    "            \"distances\": []\n",
    "        }\n",
    "        \n",
    "        final_relevant_chunks[\"documents\"] = final_relevant_chunks_cases[\"documents\"] + final_relevant_chunks_kb[\"documents\"]\n",
    "        final_relevant_chunks[\"distances\"] = final_relevant_chunks_cases[\"distances\"] + final_relevant_chunks_kb[\"distances\"]\n",
    "        \n",
    "        if len(final_relevant_chunks) == 0:\n",
    "            raise Exception(\"No relevant data found\")\n",
    "            \n",
    "        context = \"\\n\\n\\n\".join(final_relevant_chunks[\"documents\"])\n",
    "        prompt = make_prompt(final_relevant_chunks, context, question_text, input_token_limit, model)\n",
    "    else:\n",
    "      \n",
    "        if len(final_relevant_chunks_cases) + len(final_relevant_chunks_kb) == 0:\n",
    "            raise Exception(\"No relevant data found\")\n",
    "        if len(final_relevant_chunks_cases) > 0:\n",
    "            chunks_cases = \"\\n\\n\\n\".join(final_relevant_chunks_cases[\"documents\"])\n",
    "        else:\n",
    "            chunks_cases = \"Not available\"\n",
    "        \n",
    "        if len(final_relevant_chunks_kb) > 0:\n",
    "            chunks_kb = \"\\n\\n\\n\".join(final_relevant_chunks_kb[\"documents\"])\n",
    "        else:\n",
    "            chunks_kb = \"Not available\"\n",
    "        prompt = model_prompt_function(chunks_cases, chunks_kb, question_text, input_token_limit, model)\n",
    "\n",
    "    return prompt, 0, 0\n",
    "\n",
    "def prompt_template(context, question_text):\n",
    "    return (f\"You help solve current cases using information from past cases and Q&A to suggest possible solutions.\\n\\nNarrate to the user what is the most probable root cause and solution approach given the following past related cases and the current user case description. Take into account what has happened in the past cases and what solution was taken. Do not consider unique IDs in your answer, only consider the pattern, if any. Suggest both the probable root cause and a solution using the information from the past related cases data.\\n\\n\"\n",
    "          + f\"Previous Cases/Q&A:\\n\\n\"\n",
    "          + f\"{context}\\n\\n\"\n",
    "          + f\"##\\n\\n\"\n",
    "          + f\"Current Case: {question_text}\\n\\n\"\n",
    "          + f\"Probable Root Cause (if applicable) and Solution: \")\n",
    "\n",
    "def make_prompt(relevant_chunks, context, question_text, max_input_tokens, model):\n",
    "    prompt = prompt_template(context, question_text)\n",
    "\n",
    "    prompt_token_count = token_count(prompt, model)\n",
    "\n",
    "    if prompt_token_count <= max_input_tokens:\n",
    "        return prompt\n",
    "\n",
    "    print(\"exceeded input token limit, truncating context\", prompt_token_count)\n",
    "\n",
    "    distances = relevant_chunks[\"distances\"]\n",
    "    documents = relevant_chunks[\"documents\"]\n",
    "\n",
    "    #documents with the lower distance scores are included in the truncated context first\n",
    "    sorted_indices = sorted(range(len(distances)), key=lambda k: distances[k], reverse=True)\n",
    "\n",
    "    truncated_context = \"\"\n",
    "    token_count_so_far = 0\n",
    "    i = 0\n",
    "\n",
    "    while token_count_so_far <= max_input_tokens and i < len(sorted_indices):\n",
    "        doc_index = sorted_indices[i]\n",
    "        document = documents[doc_index]\n",
    "        doc_token_count = token_count(document, model)\n",
    "\n",
    "        if token_count_so_far + doc_token_count <= max_input_tokens:\n",
    "            truncated_context += document + \"\\n\\n\\n\"\n",
    "            token_count_so_far += doc_token_count\n",
    "        else:\n",
    "            remaining_tokens = max_input_tokens - token_count_so_far\n",
    "            truncated_context += document[:remaining_tokens]\n",
    "            break\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return prompt_template(truncated_context, question_text)\n",
    "\n",
    "# Token counting function\n",
    "def token_count(doc, model):\n",
    "    return model.tokenize([doc])[0].token_count\n",
    "\n",
    "def generate_answer_from_prompt(prompt, model):\n",
    "    responses = model.generate([prompt])\n",
    "    response = responses[0]\n",
    "    #print(response)\n",
    "    return response.generated_text, response.input_token_count, response.generated_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d1f4adc1-1918-4b93-a265-d491bdebc657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(html_text):\n",
    "    # Create a BeautifulSoup object to parse the HTML\n",
    "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "\n",
    "    # Extract the plain text content from the HTML\n",
    "    text_content = soup.get_text(separator=\"\\n\")\n",
    "\n",
    "    return text_content\n",
    "\n",
    "def cap_consecutive_newlines(input_str):\n",
    "    # Use a regular expression to replace consecutive newlines with a maximum of two\n",
    "    result = re.sub(r'\\n{3,}', '\\n', input_str)\n",
    "    return result\n",
    "\n",
    "def remove_extra_spaces(input_str):\n",
    "    # Use a regular expression to replace multiple spaces with a single space\n",
    "    result = re.sub(r' +', ' ', input_str)\n",
    "    return result.strip()\n",
    "\n",
    "def preprocess_text_input(txt):\n",
    "    return cap_consecutive_newlines(remove_extra_spaces(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "269bd65d-ed98-4d23-a6d9-ab46ed55a3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = AutoModel.from_pretrained('intfloat/e5-base-v2')\n",
    "embedding_tokenizer = AutoTokenizer.from_pretrained('intfloat/e5-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4f850be5-f0d5-4e36-abcc-9cb7d8ea2c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_by_index = []\n",
    "kb_by_index = []\n",
    "\n",
    "total_in_tokens_l1 = 0\n",
    "total_out_tokens_l1 = 0\n",
    "\n",
    "for index, row in challenge_question_df.iterrows():\n",
    "    question_text = preprocess_text_input(f'''Subject: {row['short_description']}\\n\\nDescription: {row['long_description']}''')\n",
    "    cases, qna = get_relevant_chunks(question_text)\n",
    "    filtered_cases, filtered_qna, in_tokens_l1, out_tokens_l1 = filter_relevant_chunks_individual(question_text, cases, qna)\n",
    "    cases_by_index.append(filtered_cases)\n",
    "    kb_by_index.append(filtered_qna)\n",
    "    total_in_tokens_l1 += in_tokens_l1\n",
    "    total_out_tokens_l1 += out_tokens_l1\n",
    "    if len(filtered_cases) + len(filtered_qna) == 0:\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0198f9a0-5036-4dfd-87ac-58176dccd6a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonzeroindices = []\n",
    "for i in range(len(challenge_question_df)):\n",
    "    if len(cases_by_index[i]) or len(kb_by_index[i]):\n",
    "        nonzeroindices.append(i)\n",
    "len(nonzeroindices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "02bed6e5-b08b-4adc-8434-fe7e1d66e114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 processed with model ibm/mpt-7b-instruct\n",
      "1 processed with model ibm/mpt-7b-instruct\n",
      "2 processed with model ibm/mpt-7b-instruct\n",
      "0 processed with model ibm/granite-13b-chat-v1\n",
      "1 processed with model ibm/granite-13b-chat-v1\n",
      "2 processed with model ibm/granite-13b-chat-v1\n",
      "0 processed with model meta-llama/llama-2-70b-chat\n",
      "1 processed with model meta-llama/llama-2-70b-chat\n",
      "2 processed with model meta-llama/llama-2-70b-chat\n"
     ]
    }
   ],
   "source": [
    "model_names = [\n",
    "    'ibm/mpt-7b-instruct',\n",
    "    'ibm/granite-13b-chat-v1',\n",
    "    'meta-llama/llama-2-70b-chat'\n",
    "]\n",
    "\n",
    "params = GenerateParams(\n",
    "    decoding_method=\"greedy\",\n",
    "    max_new_tokens=512,\n",
    "    min_new_tokens=1,\n",
    "    stream=False,\n",
    "    repetition_penalty=1.2,\n",
    "    stop_sequences=['<endoftext>','END_KEY','####','\\n\\nUser:','\\n\\nAssistant:','\\n\\n--\\n\\n']\n",
    ")\n",
    "\n",
    "prompt_constructors_by_model = {\n",
    "    'ibm/granite-13b-chat-v1': make_granite_prompt,\n",
    "    'meta-llama/llama-2-70b-chat': make_llama2_prompt\n",
    "}\n",
    "\n",
    "combined_answers_by_model = {}\n",
    "\n",
    "token_counts_by_model = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "\n",
    "    answers = []\n",
    "\n",
    "    token_counts_by_model[model_name] = {\n",
    "        \"in_total\": total_in_tokens_l1,\n",
    "        \"out_total\": total_out_tokens_l1,\n",
    "        \"in_l1\": total_in_tokens_l1,\n",
    "        \"out_l1\": total_out_tokens_l1,\n",
    "        \"in_l2\": 0,\n",
    "        \"out_l2\": 0\n",
    "    }\n",
    "\n",
    "    for i, row in challenge_question_df.iterrows():\n",
    "\n",
    "        if i in nonzeroindices:\n",
    "            if model_name in prompt_constructors_by_model:\n",
    "                ans, in_tokens, out_tokens, in_l1, out_l1, in_l2, out_l2 = get_answer_from_question_and_relevant_chunks(cases_by_index[i], kb_by_index[i], preprocess_text_input(f'''Subject: {row['short_description']}\\n\\nDescription: {row['long_description']}'''), params, assignment_group=row['assignment_group'], model_prompt_function=prompt_constructors_by_model[model_name], model_name=model_name)\n",
    "            else:\n",
    "                ans, in_tokens, out_tokens, in_l1, out_l1, in_l2, out_l2 = get_answer_from_question_and_relevant_chunks(cases_by_index[i], kb_by_index[i], preprocess_text_input(f'''Subject: {row['short_description']}\\n\\nDescription: {row['long_description']}'''), assignment_group=row['assignment_group'], params=params, model_name=model_name)\n",
    "        \n",
    "            # update token counts to track usage\n",
    "            token_counts_by_model[model_name][\"in_total\"] += in_tokens\n",
    "            token_counts_by_model[model_name][\"out_total\"] += out_tokens\n",
    "            token_counts_by_model[model_name][\"in_l1\"] += in_l1\n",
    "            token_counts_by_model[model_name][\"in_l2\"] += in_l2\n",
    "            token_counts_by_model[model_name][\"out_l1\"] += out_l1\n",
    "            token_counts_by_model[model_name][\"out_l2\"] += out_l2\n",
    "\n",
    "            # append the answer\n",
    "            answers.append(cap_consecutive_newlines(ans))\n",
    "            print(f'{i} processed with model {model_name}')\n",
    "            \n",
    "        else:\n",
    "            #print(e)\n",
    "            #print(f'{i} encountered error: {e}')\n",
    "            answers.append(\"\")\n",
    "            print(f'{i} skipped')\n",
    "        \n",
    "    combined_answers_by_model[model_name] = answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b97221a1-5a7e-48ee-a065-b1704c667526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ibm/mpt-7b-instruct': {'in_total': 10394,\n",
       "  'out_total': 461,\n",
       "  'in_l1': 8930,\n",
       "  'out_l1': 60,\n",
       "  'in_l2': 1464,\n",
       "  'out_l2': 401},\n",
       " 'ibm/granite-13b-chat-v1': {'in_total': 10574,\n",
       "  'out_total': 149,\n",
       "  'in_l1': 8930,\n",
       "  'out_l1': 60,\n",
       "  'in_l2': 1644,\n",
       "  'out_l2': 89},\n",
       " 'meta-llama/llama-2-70b-chat': {'in_total': 10767,\n",
       "  'out_total': 635,\n",
       "  'in_l1': 8930,\n",
       "  'out_l1': 60,\n",
       "  'in_l2': 1837,\n",
       "  'out_l2': 575}}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counts_by_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f703f437-d19d-45a0-9588-dce6a679bcd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8827"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_in_tokens_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c9706585-a184-49f9-b6eb-dec1b66f0cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_out_tokens_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "028e092f-502f-45ed-afa5-98364af548cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_with_answers_df = challenge_question_df.copy()\n",
    "\n",
    "for i in range(len(model_names)):\n",
    "    questions_with_answers_df[f'model{i+1}_ans'] = combined_answers_by_model[model_names[i]]\n",
    "\n",
    "for i in range(len(model_names)):\n",
    "    questions_with_answers_df[f'model{i+1}_score'] = \"\"\n",
    "\n",
    "questions_with_answers_df.to_excel(f'{account}_round1_with_answers.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be5d526-fb2e-4489-8688-bea66869d5b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
