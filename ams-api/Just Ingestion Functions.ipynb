{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059d4767-48a0-44a8-9d3d-5d188abf0b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Any, Dict, Iterable, List, Optional\n",
    "\n",
    "#import chromadb\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tqdm\n",
    "#from chromadb.api.types import EmbeddingFunction\n",
    "from dotenv import load_dotenv\n",
    "from genai import Model\n",
    "from genai.model import Credentials\n",
    "from genai.schemas import GenerateParams\n",
    "#from rouge import Rouge\n",
    "from datasets import Dataset\n",
    "#from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from chunkipy import TextChunker, TokenEstimator\n",
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "from milvus import default_server\n",
    "from pymilvus import connections, utility\n",
    "\n",
    "from torch.nn.functional import normalize\n",
    "from torch import clamp, sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e407f53f-35fc-4a8b-abef-e23bd20f0bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_genai_creds():\n",
    "    load_dotenv(override=True)\n",
    "    api_key = os.getenv(\"GENAI_KEY\", None)\n",
    "    api_url = os.getenv(\"GENAI_API\", None)\n",
    "    if api_key is None or api_url is None:\n",
    "        print(\"Either api_key or api_url is None. Please make sure your credentials are correct.\")\n",
    "    if api_url is not None:\n",
    "        api_url = api_url.rstrip(\"/\")\n",
    "    creds = Credentials(api_key, api_url)\n",
    "    return creds\n",
    "\n",
    "creds = get_genai_creds()\n",
    "if creds.api_endpoint:\n",
    "    print(f\"Your API endpoint is: {creds.api_endpoint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f932885-f111-45aa-9f90-0680e4c7b76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of supported models from the API\n",
    "models_response = requests.get(f\"{creds.api_endpoint}/models\")\n",
    "\n",
    "# Parse the JSON response\n",
    "models_data = json.loads(models_response.content)\n",
    "\n",
    "model_ids = []\n",
    "for model_n in models_data[\"results\"]:\n",
    "    print(model_n[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153090c4-212b-4915-bf5f-db80defc5dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(html_text):\n",
    "    # Create a BeautifulSoup object to parse the HTML\n",
    "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "\n",
    "    # Extract the plain text content from the HTML\n",
    "    text_content = soup.get_text(separator=\"\\n\")\n",
    "\n",
    "    return text_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337dcfd1-764d-4ac9-a055-496c4771ad4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def cap_consecutive_newlines(input_str):\n",
    "    # Use a regular expression to replace consecutive newlines with a maximum of two\n",
    "    result = re.sub(r'\\n{3,}', '\\n', input_str)\n",
    "    return result\n",
    "\n",
    "def remove_extra_spaces(input_str):\n",
    "    # Use a regular expression to replace multiple spaces with a single space\n",
    "    result = re.sub(r' +', ' ', input_str)\n",
    "    return result.strip()\n",
    "\n",
    "def preprocess_text_input(txt):\n",
    "    return cap_consecutive_newlines(remove_extra_spaces(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d350821-d83f-4f3e-8e7c-4fd9eea36a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_v1(filename):\n",
    "    if filename.endswith('.csv'):\n",
    "        psgs = pd.read_csv(filename, header=0, low_memory=False)\n",
    "    else:\n",
    "        psgs = pd.read_excel(filename)\n",
    "    return psgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414bdcc4-e613-4172-8547-4dffe99b14c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickets = load_data_v1(\"ExampleTicketData.xlsx\")\n",
    "knowledge = load_data_v1(\"ExampleKbData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ad3c98-c631-4a28-8b2d-55a1db463034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_ticket_data(tickets):\n",
    "    # can add to these lists if there are more tickets that we do not wish to consider\n",
    "    banned_additional_comments = [x for x in tickets['additional_comments'].unique() if len(str(x)) < 10]\n",
    "    banned_resolution = [x for x in tickets['resolution'].unique() if len(str(x)) < 10]\n",
    "    tickets = tickets[~(pd.isna(tickets['additional_comments']) & pd.isna(tickets['resolution']))]\n",
    "    tickets = tickets[~(tickets['additional_comments'].isin(banned_additional_comments) & tickets['resolution'].isin(banned_resolution))]\n",
    "    return tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1110079c-8386-44bd-a086-6b7668362f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickets = refine_ticket_data(tickets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7d6daa-6a47-4f88-81ad-fa7587019ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained('intfloat/e5-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained('intfloat/e5-base-v2')\n",
    "\n",
    "TOKENIZER_MAX_SIZE = 512 # may have to change this  if embedding model is changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ece7f3-f9d9-4502-9a4b-8abf18868b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTokenEstimator(TokenEstimator):\n",
    "    def __init__(self):\n",
    "        self.bert_tokenizer = AutoTokenizer.from_pretrained('intfloat/e5-base-v2')\n",
    "\n",
    "    def estimate_tokens(self, text):\n",
    "        return len(self.bert_tokenizer.encode(text))\n",
    "\n",
    "bert_token_estimator = BertTokenEstimator()\n",
    "\n",
    "text_chunker = TextChunker(TOKENIZER_MAX_SIZE, tokens=True, token_estimator=BertTokenEstimator(), overlap_percent=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff63c6da-129a-46b6-8d5d-e5298aa429f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    normalized_text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return normalized_text\n",
    "\n",
    "def remove_emails(text):\n",
    "    return re.sub(r'\\S+@\\S+', '', text)\n",
    "\n",
    "def chunkTicketData(row):\n",
    "    text = createIndexStrTickets(row)\n",
    "    return text_chunker.chunk(text)\n",
    "\n",
    "def chunkKbData(row):\n",
    "    text = createIndexStrKb(row)\n",
    "    return text_chunker.chunk(text)\n",
    "\n",
    "def convert_to_lower(inp):\n",
    "    return inp.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5608e4d-2b70-40c4-9def-04a97dc6714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createIndexStrTickets(row):\n",
    "    banned_additional_comments = [x for x in tickets['additional_comments'].unique() if len(str(x)) < 10]\n",
    "    banned_resolution = [x for x in tickets['resolution'].unique() if len(str(x)) < 10]\n",
    "    \n",
    "    result = 'Subject: ' + str(row['short_description']) + '\\n\\n'\n",
    "    result += 'Description: ' + re.sub(r\"\\+91\\s?\\d+$\", \"\", str(row['long_description']).strip(), flags=re.DOTALL) + '\\n\\n'\n",
    "    \n",
    "    if pd.notnull(row['resolution']) and row['resolution'] not in banned_resolution:\n",
    "        result += 'Resolution notes: ' + re.sub(r\"\\+91\\s?\\d+$\", \"\", str(row['resolution']).strip(), flags=re.DOTALL) + '\\n\\n'\n",
    "    if pd.notnull(row['additional_comments']) and row['additional_comments'] not in banned_additional_comments:\n",
    "        result += 'Additional Comments: ' + re.sub(r\"\\+91\\s?\\d+$\", \"\", str(row['additional_comments']).strip(), flags=re.DOTALL)\n",
    "    \n",
    "    result = preprocess_text_input(result)\n",
    "    result = remove_emails(result)\n",
    "    return remove_non_ascii(result.replace('\\r',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f27ca54-11b0-4fcc-b14f-788cc3af5290",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickets = tickets[['assignment_group', 'short_description', 'long_description', 'resolution', 'additional_comments']]\n",
    "tickets['assignment_group'] = tickets['assignment_group'].apply(convert_to_lower)\n",
    "tickets = tickets.reset_index()\n",
    "tickets = tickets.rename(columns={'index': 'ID'})\n",
    "tickets.set_index('ID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c222d35-4325-4a1f-bb47-6e9d73e792e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_chunks = tickets.apply(chunkTicketData, axis=1).explode().reset_index()\n",
    "\n",
    "ticket_chunks.columns = ['ID', 'text']\n",
    "\n",
    "# First, let's create a mapping from 'ID' to 'CLASS' in the `documents_copy` DataFrame.\n",
    "class_mapping = tickets['assignment_group'].to_dict()\n",
    "\n",
    "# Now, let's add the 'CLASS' column to the `chunks_df` DataFrame using this mapping.\n",
    "ticket_chunks['assignment_group'] = ticket_chunks['ID'].map(class_mapping)\n",
    "\n",
    "ticket_chunks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f16e1a-b73e-40e0-8291-c20f1838fefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createIndexStrKb(row):\n",
    "    result = 'Topic: ' + remove_html_tags(str(row['question'])) + '\\n\\n'\n",
    "    result += 'Answer: ' + remove_html_tags(str(row['answer'])) + '\\n\\n'\n",
    "    result += 'Tags: ' + remove_html_tags(str(row['tags']))\n",
    "    re.sub(r\"\\+91\\s\\d+\\b\", \"\", result, flags=re.DOTALL)\n",
    "    result = remove_emails(result)\n",
    "    result = preprocess_text_input(result)\n",
    "    return remove_non_ascii(result.replace('\\r',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfde9ae-80ce-431b-b276-051b0f1ac5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_chunks = knowledge.apply(chunkKbData, axis=1).explode().reset_index()\n",
    "\n",
    "knowledge_chunks.columns = ['ID', 'text']\n",
    "\n",
    "class_mapping = knowledge['tags'].to_dict()\n",
    "\n",
    "knowledge_chunks['tags'] = knowledge_chunks['ID'].map(class_mapping)\n",
    "\n",
    "knowledge_chunks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b6f853-81bf-4f32-8d71-7e061cd6bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "account = \"example\"\n",
    "\n",
    "EMBEDDING_DIMENSION = 768 # may need to change this if embedding model changes\n",
    "COLLECTION_NAME = f\"AMS_{account}\"\n",
    "connections.connect(host='127.0.0.1', port=default_server.listen_port)\n",
    "\n",
    "# Check if the server is ready.\n",
    "print(utility.get_server_version())\n",
    "\n",
    "# Remove collection if it already exists\n",
    "if utility.has_collection(COLLECTION_NAME):\n",
    "    utility.drop_collection(COLLECTION_NAME)\n",
    "\n",
    "# Create collection which includes the id, title, and embedding.\n",
    "fields = [\n",
    "    FieldSchema(name='id', dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "    FieldSchema(name='ticket_id', dtype=DataType.INT64),\n",
    "    FieldSchema(name='assignment_id', dtype=DataType.VARCHAR, max_length=128),\n",
    "    FieldSchema(name='type', dtype=DataType.VARCHAR, max_length=2),\n",
    "    FieldSchema(name='chunk', dtype=DataType.VARCHAR, max_length=6000),\n",
    "    FieldSchema(name='chunk_embedding', dtype=DataType.FLOAT_VECTOR, dim=EMBEDDING_DIMENSION)\n",
    "]\n",
    "\n",
    "schema = CollectionSchema(fields=fields)\n",
    "collection = Collection(name=COLLECTION_NAME, schema=schema)\n",
    "\n",
    "# Create an FLAT index for collection.\n",
    "index_params = {\n",
    "    'metric_type':'IP',\n",
    "    'index_type':\"FLAT\"\n",
    "}\n",
    "\n",
    "collection.create_index(field_name=\"chunk_embedding\", index_params=index_params)\n",
    "collection.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4c743a-7d72-4b24-8bf9-2d32be3e4a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_ticket_data(batch):\n",
    "    results = tokenizer([\"passage: \" + x for x in batch['text']], add_special_tokens = True, truncation = True, padding = \"max_length\", return_attention_mask = True, return_tensors = \"pt\")\n",
    "    batch['input_ids'] = results['input_ids']\n",
    "    batch['token_type_ids'] = results['token_type_ids']\n",
    "    batch['attention_mask'] = results['attention_mask']\n",
    "    return batch\n",
    "\n",
    "TOKENIZATION_BATCH_SIZE = 256 # may need to lower this for larger embedding models\n",
    "\n",
    "ticket_dataset = Dataset.from_pandas(ticket_chunks)\n",
    "\n",
    "# Generate the tokens for each entry.\n",
    "ticket_dataset = ticket_dataset.map(tokenize_ticket_data, batch_size=TOKENIZATION_BATCH_SIZE, batched=True)\n",
    "# Set the ouput format to torch so it can be pushed into embedding model\n",
    "ticket_dataset.set_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask'], output_all_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35a0ef0-ce38-41a0-ae4e-781994243b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_BATCH_SIZE = 16\n",
    "\n",
    "# Embed the tokenized data and take the mean pool with respect to attention mask of hidden layer.\n",
    "def embed(batch):\n",
    "    sentence_embs = model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                token_type_ids=batch['token_type_ids'],\n",
    "                attention_mask=batch['attention_mask']\n",
    "                )[0]\n",
    "    input_mask_expanded = batch['attention_mask'].unsqueeze(-1).expand(sentence_embs.size()).float()\n",
    "    batch['question_embedding'] = sum(sentence_embs * input_mask_expanded, 1) / clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return batch\n",
    "\n",
    "ticket_dataset = ticket_dataset.map(embed, remove_columns=['input_ids', 'token_type_ids', 'attention_mask'], batched = True, batch_size=EMBEDDING_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd4a89d-d660-46ca-9821-5b60f599b5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e211e50-b012-4b52-a8ac-ba6f376453fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_tickets(batch):\n",
    "    insertable = [\n",
    "        batch['ID'].tolist(),\n",
    "        [x.lower().strip() for x in batch['assignment_group']],\n",
    "        ['td' for _ in range(len(batch['text']))],\n",
    "        batch['text'],\n",
    "        normalize(batch['question_embedding'], dim=1).tolist()\n",
    "    ]\n",
    "    collection.insert(insertable)\n",
    "\n",
    "ticket_dataset.map(insert_tickets, batched=True, batch_size=64)\n",
    "collection.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe7bc3e-5920-4c61-9b96-f62380115c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_kb_chunk_data(batch):\n",
    "    results = tokenizer([\"passage: \" + x for x in batch['text']], add_special_tokens = True, truncation = True, padding = \"max_length\", return_attention_mask = True, return_tensors = \"pt\")\n",
    "    batch['input_ids'] = results['input_ids']\n",
    "    batch['token_type_ids'] = results['token_type_ids']\n",
    "    batch['attention_mask'] = results['attention_mask']\n",
    "    return batch\n",
    "\n",
    "TOKENIZATION_BATCH_SIZE = 256\n",
    "\n",
    "knowledge_dataset = Dataset.from_pandas(knowledge_chunks)\n",
    "\n",
    "# Generate the tokens for each entry.\n",
    "knowledge_dataset = knowledge_dataset.map(tokenize_kb_chunk_data, batch_size=TOKENIZATION_BATCH_SIZE, batched=True)\n",
    "# Set the ouput format to torch so it can be pushed into embedding model\n",
    "knowledge_dataset.set_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask'], output_all_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb96f485-2c6e-4a6e-a652-0172ae26e17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_dataset = knowledge_dataset.map(embed, remove_columns=['input_ids', 'token_type_ids', 'attention_mask'], batched = True, batch_size=EMBEDDING_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032c1b11-56df-474d-9f54-228ac904f634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_kb(batch):\n",
    "    insertable = [\n",
    "        [-1 for x in batch['ID']],\n",
    "        [x.lower() for x in batch['tags']],\n",
    "        ['kb' for _ in range(len(batch['text']))],\n",
    "        batch['text'],\n",
    "        normalize(torch.FloatTensor(batch['question_embedding']), dim=1).tolist()\n",
    "    ]    \n",
    "    collection.insert(insertable)\n",
    "    \n",
    "knowledge_dataset.map(insert_kb, batched=True, batch_size=64)\n",
    "collection.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e9738b-7077-44ff-8720-ba34b758f4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.num_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f052f8ca-57a0-4d62-9193-4fa79f4b5a23",
   "metadata": {},
   "source": [
    "# Text Input KB Files functions follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9233b017-a29f-4d53-8c48-a558b975291d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "from docx import Document\n",
    "import os\n",
    "from pdfminer.high_level import extract_text\n",
    "from tika import parser\n",
    "\n",
    "def extract_text_from_docx(docx_file):\n",
    "    document = Document(docx_file)\n",
    "    result = []\n",
    "    \n",
    "    for paragraph in document.paragraphs:\n",
    "        result.append(paragraph.text)\n",
    "    \n",
    "    return \"\\n\".join(result)\n",
    "\n",
    "def extract_text_from_docx(docx_file):\n",
    "    doc = docx.Document(docx_file)\n",
    "    text = \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])\n",
    "    return text\n",
    "\n",
    "def append_to_text_instances(text_instances, text):\n",
    "    text_instances.append(text)\n",
    "\n",
    "def extract_text_from_pptx(input_pptx_file):\n",
    "    parsed = parser.from_file(input_pptx_file)\n",
    "    return parsed[\"content\"]\n",
    "\n",
    "processed_base_filenames = set()\n",
    "folder_path = \"adani_source_files/GenAI/ISU/\"  # Replace with the folder containing source files\n",
    "text_instances = []\n",
    "\n",
    "def process_file(file_path):\n",
    "    base_filename, file_extension = os.path.splitext(os.path.basename(file_path))\n",
    "\n",
    "    if base_filename not in processed_base_filenames:\n",
    "        if file_extension.lower() == \".docx\":\n",
    "            text = extract_text_from_docx(file_path)\n",
    "            append_to_text_instances(text_instances, text)\n",
    "        elif file_extension.lower() == \".pdf\":\n",
    "            text = extract_text(file_path)\n",
    "            append_to_text_instances(text_instances, text)\n",
    "        elif file_extension.lower() == \".pptx\":\n",
    "            text = extract_text_from_pptx(file_path)\n",
    "            append_to_text_instances(text_instances, text)\n",
    "        \n",
    "        processed_base_filenames.add(base_filename) # prevent duplicate files from being processed with diff extensions\n",
    "\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        if file.lower().endswith((\".docx\", \".pdf\")):\n",
    "            process_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20378e11-cd06-47a0-85a3-4a382aff89d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = []\n",
    "\n",
    "for text_instance in text_instances:\n",
    "    chunks = text_chunker.chunk(text_instance)\n",
    "    for chunk in chunks:\n",
    "        text_chunks.append(chunk)\n",
    "\n",
    "text_chunks_df = pd.DataFrame()\n",
    "text_chunks_df[\"text\"] = text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd95ad7-289f-48c3-8fa8-69d6fc143dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def tokenize_kb_chunk_data(batch):\n",
    "    results = tokenizer([\"passage: \" + x for x in batch['text']], add_special_tokens = True, truncation = True, padding = \"max_length\", return_attention_mask = True, return_tensors = \"pt\")\n",
    "    batch['input_ids'] = results['input_ids']\n",
    "    batch['token_type_ids'] = results['token_type_ids']\n",
    "    batch['attention_mask'] = results['attention_mask']\n",
    "    return batch\n",
    "\n",
    "TOKENIZATION_BATCH_SIZE = 512\n",
    "\n",
    "dataset_chunks = Dataset.from_pandas(text_chunks_df)\n",
    "\n",
    "dataset_chunks = dataset_chunks.map(tokenize_kb_chunk_data, batch_size=TOKENIZATION_BATCH_SIZE, batched=True)\n",
    "dataset_chunks.set_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask'], output_all_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccabde0e-6d05-4002-afc6-f353ffef54f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import normalize\n",
    "from torch import clamp, sum\n",
    "\n",
    "def embed(batch):\n",
    "    for key in ['input_ids', 'token_type_ids', 'attention_mask']:\n",
    "        batch[key] = batch[key]\n",
    "    sentence_embs = model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                token_type_ids=batch['token_type_ids'],\n",
    "                attention_mask=batch['attention_mask']\n",
    "                )[0]\n",
    "    input_mask_expanded = batch['attention_mask'].unsqueeze(-1).expand(sentence_embs.size()).float()\n",
    "    batch['question_embedding'] = sum(sentence_embs * input_mask_expanded, 1) / clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return batch\n",
    "\n",
    "dataset_chunks = dataset_chunks.map(embed, remove_columns=['input_ids', 'token_type_ids', 'attention_mask'], batched = True, batch_size=EMBEDDING_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b64320c-2561-4919-9e73-a01bc66bb59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import normalize\n",
    "from torch import clamp, sum\n",
    "\n",
    "def insert_text_chunk(batch):\n",
    "\n",
    "    insertable = [\n",
    "        [-1 for x in range(len(batch['text']))],\n",
    "        ['' for x in range(len(batch['text']))],\n",
    "        ['kb' for _ in range(len(batch['text']))],\n",
    "        ['' for x in range(len(batch['text']))],\n",
    "        batch['text'], # chunk itself - raw text\n",
    "        normalize(batch['question_embedding'], dim=1).tolist() # embedding of the chunk - vector representation (for searching)\n",
    "    ]\n",
    "    collection.insert(insertable)\n",
    "\n",
    "data_dataset_chunks.map(insert_text_chunk, batched=True, batch_size=256)\n",
    "collection.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
